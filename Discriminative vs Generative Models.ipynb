{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative vs Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(mvtnorm) # for multivariate Gaussian data\n",
    "library(ggplot2) # for plots\n",
    "library(reshape2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train_data <- read.csv('Data//1E_train.csv')\n",
    "test_data <- read.csv('Data//1E_test.csv')\n",
    "\n",
    "D <- 2\n",
    "\n",
    "train.len <- dim(train_data)[1]\n",
    "test.len <- dim(test_data)[1]\n",
    "\n",
    "# separate labels and features from data set\n",
    "train.index <- sample(1:train.len,train.len)\n",
    "train.data <- train_data[train.index,  1:D]\n",
    "train.label <- train_data[train.index, 'y']\n",
    "test.data <- test_data[, 1:D]\n",
    "test.label <- test_data[, 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian classifier\n",
    "BC.classfier  <- function(train, test) {\n",
    "    \n",
    "    train.len <- nrow(train)\n",
    "    # obtain train and test with labels\n",
    "    train.data <- train[, 1:D]\n",
    "    train.label <- train[, 'y']\n",
    "    \n",
    "    test.data  <- test[,1:D]\n",
    "    test.label  <- test[,\"y\"]\n",
    "    \n",
    "    # Class probabilities:\n",
    "    p0.hat <- sum(train.label==1)/nrow(train.data) # number of samples in class 0 divided by the training data size\n",
    "    p1.hat <- sum(train.label==-1)/nrow(train.data) # number of samples in class 1 divided by the training data size\n",
    "    \n",
    "    # calculate mean\n",
    "    mu0.hat <- colMeans(train.data[train.label==c0,])\n",
    "    mu1.hat <- colMeans(train.data[train.label==c1,])\n",
    "\n",
    "    # calculate class covariance matrices\n",
    "    sigma0.hat <- var(train.data[train.label==c0,])\n",
    "    sigma1.hat <- var(train.data[train.label==c1,])\n",
    "    \n",
    "    # shared covariance matrix\n",
    "    sigma.hat <- p0.hat * sigma0.hat + p1.hat * sigma1.hat \n",
    "    \n",
    "    # calculate posteriors\n",
    "    posterior0 <- p0.hat*dmvnorm(x=train.data, mean=mu0.hat, sigma=sigma.hat)\n",
    "    posterior1 <- p1.hat*dmvnorm(x=train.data, mean=mu1.hat, sigma=sigma.hat)\n",
    "\n",
    "    # calculate predictions\n",
    "    train.predict <- ifelse(posterior0 > posterior1, c0, c1)\n",
    "    test.predict <- ifelse(p0.hat*dmvnorm(x=test.data, mean=mu0.hat, sigma=sigma.hat) > p1.hat*dmvnorm(x=test.data, mean=mu1.hat, sigma=sigma.hat), 1, -1)\n",
    "\n",
    "    # test error\n",
    "    test.error  <- sum(test.label!=test.predict)/nrow(test.data)*100\n",
    "    \n",
    "    return(test.error)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that predicts class labels\n",
    "predict <- function(w, X, c0, c1){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(ifelse(sig>0.5, c1,c0))\n",
    "}\n",
    "    \n",
    "# function that calculate a cost function\n",
    "cost <- function (w, X, T, c0){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(sum(ifelse(T==c0, 1-sig, sig)))\n",
    "}\n",
    "\n",
    "# Sigmoid function \n",
    "sigmoid <- function(w, x){\n",
    "    return(1.0/(1.0+exp(-w%*%t(cbind(1,x)))))    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.classifer  <- function(train, test){\n",
    "     # get train and test dataset with labels   \n",
    "    train.data <- train[, 1:D]\n",
    "    train.label <- train[, 'y']\n",
    "    \n",
    "    test.data  <- test[,1:D]\n",
    "    test.label  <- test[,\"y\"]\n",
    "    \n",
    "    # Initializations\n",
    "    train.len <- nrow(train.data) # length of the train data\n",
    "    tau.max <- 10*train.len # maximum number of iterations\n",
    "    eta <- 0.01 # learning rate\n",
    "    epsilon <- 0.01 # a threshold on the cost (to terminate the process)\n",
    "    tau <- 1# iteration counter\n",
    "    terminate <- FALSE \n",
    "\n",
    "    ## Just a few name/type conversion to make the rest of the code easy to follow\n",
    "    X <- as.matrix(train.data)       # rename just for conviniance\n",
    "    T <- ifelse(train.label==c0,0,1) # rename just for conviniance\n",
    "\n",
    "    W <- matrix(,nrow=tau.max, ncol=(ncol(X)+1)) # to be used to store the estimated coefficients\n",
    "    \n",
    "    W[1,] <- runif(ncol(W))# initial weight\n",
    "\n",
    "    Y <- sigmoid(W[1,],X) # project data using the sigmoid function (just for convenient) \n",
    "\n",
    "    costs <- data.frame('tau'=1:tau.max)# to be used to trace the cost in each iteration\n",
    "    costs[1, 'cost'] <- cost(W[1,],X,T, c0)\n",
    "    \n",
    "    # we use SGD to learn the weight vector\n",
    "    while(!terminate){\n",
    "        \n",
    "        # check termination criteria:\n",
    "        terminate <- tau >= tau.max | cost(W[tau,],X,T, c0)<=epsilon\n",
    "        \n",
    "        # shuffle data:\n",
    "        train.index <- sample(1:train.len, train.len, replace = FALSE)\n",
    "        X <- X[train.index,]\n",
    "        T <- T[train.index]\n",
    "        \n",
    "        # for each datapoint:\n",
    "        for (i in 1:train.len){\n",
    "            # check termination criteria:\n",
    "            if (tau >= tau.max | cost(W[tau,],X,T, c0) <=epsilon) {terminate<-TRUE;break}\n",
    "            Y <- sigmoid(W[tau,],X)\n",
    "            \n",
    "            # Update the weights\n",
    "            W[(tau+1),] <- W[tau,] - eta * (Y[i]-T[i]) * cbind(1, t(X[i,]))\n",
    "            \n",
    "            # record the cost:\n",
    "            costs[(tau+1), 'cost'] <- cost(W[tau,],X,T, c0)\n",
    "            \n",
    "            # update the counter:\n",
    "            tau <- tau + 1\n",
    "            \n",
    "            # decrease learning rate:\n",
    "            eta = eta * 0.999\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # remove the NaN tail of the vector\n",
    "    costs <- costs[1:tau, ] \n",
    "    \n",
    "    # the final result is:\n",
    "    w <- W[tau,]\n",
    "    \n",
    "    # return error rate\n",
    "    test.error  <- sum(predict(w,test.data,c0,c1)!=test.label)/nrow(test.data)*100\n",
    "    \n",
    "    return(test.error)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the first 5 data points from the training set, train a BC and a LR model,increase the size of training set (5 data points at a time), retrain the models and calculate their training and testing errors until all training data points are used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class labels\n",
    "c0 <- 1; c1 <- -1 \n",
    "\n",
    "train.len <- nrow(train_data) # size of training dataset \n",
    "sequence <- seq(5,train.len,5) # sequence of train\n",
    "count <- length(sequence)  # number of expts\n",
    "\n",
    "# test error dataframe \n",
    "test.error <- data.frame(matrix(ncol = 0, nrow = 0))\n",
    "\n",
    "train.index <- sample(1:train.len, train.len, replace = FALSE)\n",
    "train <- train_data[train.index,]\n",
    "\n",
    "# Set errors\n",
    "for (size in sequence){\n",
    "    \n",
    "    # calculate errors for both classifier\n",
    "    bc.error  <- BC.classfier(train[1:size,], test_data)\n",
    "    lg.error  <- LR.classifer(train[1:size,], test_data)\n",
    "    \n",
    "    # set testing errors\n",
    "    test.error[size/5, 'Bayesian Classifier'] <- bc.error\n",
    "    test.error[size/5, 'Logistic Regression'] <- lg.error\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return error\n",
    "test.error <- cbind(data.frame(\"Dataset_Size\"=sequence), test.error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Dataset_Size</th><th scope=col>variable</th><th scope=col>value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 5                 </td><td>Bayesian Classifier</td><td>24.8               </td></tr>\n",
       "\t<tr><td>10                 </td><td>Bayesian Classifier</td><td> 1.8               </td></tr>\n",
       "\t<tr><td>15                 </td><td>Bayesian Classifier</td><td> 1.8               </td></tr>\n",
       "\t<tr><td>20                 </td><td>Bayesian Classifier</td><td> 1.6               </td></tr>\n",
       "\t<tr><td>25                 </td><td>Bayesian Classifier</td><td> 2.0               </td></tr>\n",
       "\t<tr><td>30                 </td><td>Bayesian Classifier</td><td> 2.0               </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " Dataset\\_Size & variable & value\\\\\n",
       "\\hline\n",
       "\t  5                  & Bayesian Classifier & 24.8               \\\\\n",
       "\t 10                  & Bayesian Classifier &  1.8               \\\\\n",
       "\t 15                  & Bayesian Classifier &  1.8               \\\\\n",
       "\t 20                  & Bayesian Classifier &  1.6               \\\\\n",
       "\t 25                  & Bayesian Classifier &  2.0               \\\\\n",
       "\t 30                  & Bayesian Classifier &  2.0               \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Dataset_Size | variable | value | \n",
       "|---|---|---|---|---|---|\n",
       "|  5                  | Bayesian Classifier | 24.8                | \n",
       "| 10                  | Bayesian Classifier |  1.8                | \n",
       "| 15                  | Bayesian Classifier |  1.8                | \n",
       "| 20                  | Bayesian Classifier |  1.6                | \n",
       "| 25                  | Bayesian Classifier |  2.0                | \n",
       "| 30                  | Bayesian Classifier |  2.0                | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Dataset_Size variable            value\n",
       "1  5           Bayesian Classifier 24.8 \n",
       "2 10           Bayesian Classifier  1.8 \n",
       "3 15           Bayesian Classifier  1.8 \n",
       "4 20           Bayesian Classifier  1.6 \n",
       "5 25           Bayesian Classifier  2.0 \n",
       "6 30           Bayesian Classifier  2.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unpivot view of errors per batch\n",
    "errors.m  <- melt(test.error, id=\"Dataset_Size\")\n",
    "head(errors.m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAP1BMVEUAAAAAv8QzMzNNTU1o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\niUhJAiQiJQmQiJQkQCJSkgD5fyZ0MtsXUR4GAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# error plots \n",
    "ggplot(errors.m, aes(x=Dataset_Size, y=value, color=variable)) + \n",
    "    geom_line(size=1) + \n",
    "    labs(title = \"Error Rate for BC Classifer and LR Classifer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What does happen for each classifier when the number of training data points is increased?\n",
    "\n",
    "For logistic regression, When the number of training data increases, the testing error decreases and then remains almost constant after the size of training dataset is large enough. While for Bayesian Classifier, the testing error decreases with increase in size of training data, however, after some value of training dataset size, the error increases. Hence, for larger values of training dataset size, we see increased error.\n",
    "\n",
    "b. Which classifier is best suited when the training set is small, and which is best suited when the training set is big?\n",
    "\n",
    "When training dataset is small we can see that bayesian classifier gives less testing error so Bayesian CLassifier is preferred, while for larger size of training data, Logistic regression is preffered.\n",
    " \n",
    "c. Justify your observations in previous questions (III.a & III.b) by providing some speculations and possible reasons.\n",
    "\n",
    "The Bayesian Classifier assumes conditional independence for every feature, so classifier expects features to be independent. For less amount of training data, the condition of independence might hold True with the help of priors which results in less error, however, when more training data is used, it is less probable for model to maintain the condition of independece (As in reality features seems dependent on each other **-0.64 correlation**). Also, Generative models assume the p(x|y) follows some particular distribution which may not be true in reality. This model has more parameters to learn and its complexity increases with increase in data size, hence, we see increase in testing error after some value of N(size).\n",
    "\n",
    "On the other hand, Logistic regression learns the probability of a data point belonging to a particular class, which improves with increase in training data size, as . Logistic regresssion being discriminative model, doesn't make any assumption about the data (unlike Bayesian Classifier). Hence, these models generalize well on testing data if it is trained on sufficient number of training data, and we can see this from the above plot (less testing error on larger training dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>x1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>x2</th><td>-0.6453937</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & x1\\\\\n",
       "\\hline\n",
       "\tx2 & -0.6453937\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | x1 | \n",
       "|---|\n",
       "| x2 | -0.6453937 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   x1        \n",
       "x2 -0.6453937"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cor(train_data['x2'], train_data['x1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " -1   1 \n",
       "170 330 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(train_data['y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
